media_file_path = "/Users/hosseinhajmirbaba/Desktop/Txt_Speech/test.wav"
media_file = open(media_file_path, "rb")


response = openai.Audio.transcribe(
    model=model_id,
    file=media_file,
    response_format="json"
)

print(response)

##################################################

def gptPrompt(audio):
    prompt = transcribeAudio(audio)
    response = openai.Completion.create(
        engine="davinci",
        message="Human: Hello, who are you?\nAI:",
        prompt=prompt,
        temperature=0.9,
        max_tokens=150,
        top_p=1,
        frequency_penalty=0.0,
        presence_penalty=0.6,
        stop=["\n", " Human:", " AI:"]
    )

    answer = response["choices"][0]["text"]
    return answer

##################################################
# Recording
print("Recording started...")
recorded_data = sd.rec(int(duration * fs), samplerate=fs, channels=channels)
sd.wait()  # Wait for the recording to finish
print("Recording finished.")

# Convert data to PCM format and save as WAV file
pcm_data = (recorded_data * np.iinfo(np.int16).max).astype(np.int16).tobytes()
write_wav(pcm_data, output_filename, fs)
print(f"Voice recording saved as {output_filename}.")


.then(data => {
  const messageContainer = document.createElement("div");

  const userMessageElem = document.createElement("p");
  userMessageElem.className = "user-message";
  userMessageElem.innerHTML = "User: " + data.user_prompt;
  messageContainer.appendChild(userMessageElem);

  const assistantMessageElem = document.createElement("p");
  assistantMessageElem.className = "assistant-message";
  assistantMessageElem.innerHTML = "Donna: " + data.response;
  messageContainer.appendChild(assistantMessageElem);

  const chatElem = document.querySelector("#chat");
  chatElem.appendChild(messageContainer);
  
  // Play the AI response audio
  const audioData = new Audio("data:audio/mp3;base64," + data.response_audio);
  audioData.play();

  recordBtn.disabled = false;
})



#####correct record part
@app.route("/record", methods=["POST"])
def record_voice():
    audio_data = request.files["audio_data"]
    audio_data.save("voice_recording.webm")

    # Convert webm to wav
    webm_audio = AudioSegment.from_file("voice_recording.webm", format="webm")
    webm_audio.export("voice_recording.wav", format="wav")

    prompt = transcribeAudio("voice_recording.wav")

    response = gptPrompt(prompt)
    return jsonify({"response": response, "userPrompt": prompt})


def synthesize_speech(text):
    polly_client = boto3.client('polly', region_name='ca-central-1')

    try:
        response = polly_client.synthesize_speech(
            VoiceId='Joanna',
            OutputFormat='mp3',
            Text=text
        )
    except (BotoCoreError, ClientError) as error:
        print(error)
        raise error

    if 'AudioStream' in response:
        with closing(response['AudioStream']) as stream:
            audio_data = stream.read()

        return audio_data
    else:
        raise ValueError("Failed to synthesize audio")